{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Synapse Notebook for Data Transformation\n",
    "\n",
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "# This initializes the Spark environment for data processing.\n",
    "spark = SparkSession.builder.appName(\"TransformSalesData\").getOrCreate()\n",
    "\n",
    "# Step 2: Load Raw Data\n",
    "# Specify the path to the raw data stored in Azure Data Lake.\n",
    "raw_data_path = \"abfss://raw@<storage_account>.dfs.core.windows.net/sales_data/\"\n",
    "raw_df = spark.read.json(raw_data_path)  # Read raw data in JSON format\n",
    "\n",
    "# Display a sample of the raw data for validation\n",
    "raw_df.show()\n",
    "\n",
    "# Step 3: Perform Transformation\n",
    "# Aggregate total sales by region and product\n",
    "sales_summary = raw_df.groupBy(\"region\", \"product\").sum(\"amount\")\n",
    "\n",
    "# Rename the aggregated column for clarity\n",
    "sales_summary = sales_summary.withColumnRenamed(\"sum(amount)\", \"total_sales\")\n",
    "\n",
    "# Display the transformed data\n",
    "sales_summary.show()\n",
    "\n",
    "# Step 4: Save Transformed Data\n",
    "# Specify the path to save the processed data\n",
    "processed_data_path = \"abfss://processed@<storage_account>.dfs.core.windows.net/sales_summary/\"\n",
    "sales_summary.write.mode(\"overwrite\").parquet(processed_data_path)\n",
    "\n",
    "# Log completion\n",
    "print(\"Data transformation completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
